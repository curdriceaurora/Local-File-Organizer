---
name: hash-based-exact-duplicate-detection
title: Implement hash-based exact duplicate detection
status: open
created: 2026-01-21T05:53:16Z
updated: 2026-01-26T00:52:32Z
size: M
hours: 16
parallel: true
dependencies: []
github: https://github.com/curdriceaurora/Local-File-Organizer/issues/215
---

# Task 001: Implement hash-based exact duplicate detection

## Description

Implement a robust hash-based duplicate file detection system that can identify exact duplicates across the file system using cryptographic hash functions (MD5/SHA256). The system should build an efficient index of file hashes, detect duplicates, and provide a safe user confirmation interface before any deletion operations. Include a safe mode with automatic backups to prevent accidental data loss.

## Acceptance Criteria

- [ ] MD5 and SHA256 hash computation implemented for files of all sizes
- [ ] Efficient duplicate file index built using hash lookups
- [ ] User confirmation interface displays duplicate groups with file details (path, size, modification time)
- [ ] Safe mode creates backups before deletion
- [ ] Command-line interface supports dry-run mode
- [ ] Performance optimization for large files (chunked reading)
- [ ] Progress indicators for hash computation on large file sets
- [ ] Configurable hash algorithm selection (MD5 vs SHA256)
- [ ] Duplicate detection report with statistics (total duplicates, potential space savings)
- [ ] Integration tests covering various file scenarios

## Technical Details

### Implementation Approach

1. **Hash Computation Module**
   - Implement `FileHasher` class in `file_organizer/services/deduplication/hasher.py`
   - Support both MD5 (faster) and SHA256 (more secure) algorithms
   - Use chunked reading for memory efficiency with large files
   - Include file size as quick pre-filter before hashing

2. **Duplicate Index**
   - Create `DuplicateIndex` class to maintain hash-to-files mapping
   - Use dictionary with hash as key, list of file paths as value
   - Store metadata: file size, modification time, access time

3. **User Confirmation Interface**
   - Interactive CLI prompt showing duplicate groups
   - Display file metadata for informed decision making
   - Options: keep oldest, keep newest, keep largest, manual selection
   - Batch confirmation for multiple duplicate groups

4. **Safe Mode & Backups**
   - Create `.file_organizer_backups/` directory
   - Copy files to backup before deletion
   - Maintain backup manifest with timestamps
   - Cleanup command for old backups

### File Structure
```
file_organizer/services/deduplication/
├── __init__.py
├── hasher.py           # FileHasher class
├── index.py            # DuplicateIndex class
├── detector.py         # DuplicateDetector orchestrator
└── backup.py           # BackupManager class
```

### Key Classes

```python
class FileHasher:
    def compute_hash(self, file_path: Path, algorithm: str = "sha256") -> str
    def compute_batch(self, file_paths: List[Path]) -> Dict[Path, str]

class DuplicateIndex:
    def add_file(self, file_path: Path, file_hash: str, metadata: dict)
    def get_duplicates(self) -> Dict[str, List[Path]]
    def get_statistics(self) -> dict

class DuplicateDetector:
    def scan_directory(self, directory: Path) -> DuplicateIndex
    def remove_duplicates(self, index: DuplicateIndex, strategy: str)
```

## Dependencies

- Python standard libraries: `hashlib`, `pathlib`, `shutil`
- No external dependencies for core functionality
- Optional: `tqdm` for progress bars
- Integration with existing `FileOrganizer` service

## Effort Estimate

- **Size**: Medium (M)
- **Estimated Hours**: 16 hours
- **Breakdown**:
  - Hash computation module: 4 hours
  - Duplicate index implementation: 3 hours
  - User confirmation interface: 3 hours
  - Safe mode and backup system: 4 hours
  - Testing and documentation: 2 hours

## Definition of Done

- [ ] All acceptance criteria met
- [ ] Unit tests written with >90% coverage
- [ ] Integration tests verify end-to-end duplicate detection
- [ ] Performance tests validate handling of 10,000+ files
- [ ] Documentation updated with usage examples
- [ ] CLI commands documented in user guide
- [ ] Code reviewed and approved
- [ ] No regression in existing functionality
