---
name: semantic-similarity-document-deduplication
title: Add semantic similarity for document deduplication
status: open
created: 2026-01-21T05:53:16Z
updated: 2026-01-21T06:00:27Z
size: L
hours: 24
parallel: false
dependencies: [46, 47]
---

# Task 003: Add semantic similarity for document deduplication

## Description

Implement semantic similarity detection for document deduplication using text extraction and embedding generation. This system analyzes document content to identify semantically similar files (e.g., different versions of the same document, reformatted copies) beyond exact duplicates. Use scikit-learn for TF-IDF vectorization and cosine similarity, with configurable thresholds. Provide comprehensive storage reclamation reporting.

## Acceptance Criteria

- [ ] Text extraction from multiple document formats (PDF, DOCX, TXT, RTF, ODT)
- [ ] TF-IDF vectorization using scikit-learn
- [ ] Cosine similarity computation for document pairs
- [ ] Configurable similarity threshold (default: 0.85)
- [ ] Semantic duplicate detection and grouping
- [ ] Integration with hash-based duplicate detection (Task 001)
- [ ] Storage reclamation reporting with before/after statistics
- [ ] Preview of similar documents for user verification
- [ ] Batch processing with progress tracking
- [ ] Support for large document collections (1,000+ files)
- [ ] Export duplicate reports to CSV/JSON

## Technical Details

### Implementation Approach

1. **Text Extraction Module**
   - Create `DocumentExtractor` class in `file_organizer/services/deduplication/document_dedup.py`
   - Use `PyPDF2` for PDF text extraction
   - Use `python-docx` for DOCX files
   - Plain text handling for TXT files
   - Fallback to OCR for scanned documents (optional)

2. **Embedding Generation**
   - Implement `DocumentEmbedder` class
   - TF-IDF vectorization with scikit-learn
   - Configurable parameters: max_features, ngram_range, min_df
   - Handle multilingual documents (UTF-8 encoding)
   - Cache embeddings for performance

3. **Similarity Analysis**
   - Implement `SemanticAnalyzer` class
   - Cosine similarity computation using sparse matrices
   - Efficient pairwise comparison for large datasets
   - Similarity threshold configuration
   - Group documents by similarity clusters

4. **Integration & Reporting**
   - Combine with exact duplicate detection from Task 001
   - Unified reporting interface
   - Storage reclamation calculator
   - Visual reports with charts (optional)

### File Structure
```
file_organizer/services/deduplication/
├── __init__.py
├── document_dedup.py   # DocumentDeduplicator orchestrator
├── extractor.py        # DocumentExtractor class
├── embedder.py         # DocumentEmbedder class
├── semantic.py         # SemanticAnalyzer class
└── reporter.py         # StorageReporter class
```

### Key Classes

```python
class DocumentExtractor:
    def extract_text(self, file_path: Path) -> str
    def extract_batch(self, file_paths: List[Path]) -> Dict[Path, str]
    def supports_format(self, file_path: Path) -> bool

class DocumentEmbedder:
    def __init__(self, max_features: int = 5000, ngram_range: Tuple = (1, 2))
    def fit_transform(self, documents: List[str]) -> np.ndarray
    def transform(self, document: str) -> np.ndarray

class SemanticAnalyzer:
    def __init__(self, threshold: float = 0.85)
    def compute_similarity(self, doc1_vector: np.ndarray, doc2_vector: np.ndarray) -> float
    def find_similar_documents(self, embeddings: np.ndarray, paths: List[Path]) -> Dict
    def cluster_by_similarity(self, duplicates: Dict) -> List[List[Path]]

class StorageReporter:
    def calculate_reclamation(self, duplicate_groups: Dict) -> dict
    def generate_report(self, output_format: str = "text") -> str
    def export_to_csv(self, duplicate_groups: Dict, output_path: Path)
```

### Dependencies

- `scikit-learn`: TF-IDF vectorization and cosine similarity
- `PyPDF2`: PDF text extraction
- `python-docx`: DOCX text extraction
- `numpy`: Numerical operations
- Optional: `pytesseract` for OCR
- Optional: `matplotlib` for visual reports

## Effort Estimate

- **Size**: Large (L)
- **Estimated Hours**: 24 hours
- **Breakdown**:
  - Text extraction implementation: 5 hours
  - TF-IDF and embedding generation: 4 hours
  - Similarity analysis and clustering: 5 hours
  - Integration with Tasks 001 and 002: 4 hours
  - Storage reporting and export: 3 hours
  - Testing and optimization: 2 hours
  - Documentation: 1 hour

## Dependencies

**Depends on:**
- **Task 001**: Hash-based exact duplicate detection (for integration)
- **Task 002**: Perceptual hashing for images (for unified deduplication system)

This task integrates the complete deduplication pipeline, combining:
- Exact duplicates (hash-based)
- Similar images (perceptual hashing)
- Similar documents (semantic analysis)

## Definition of Done

- [ ] All acceptance criteria met
- [ ] Text extraction works for PDF, DOCX, TXT, RTF, ODT formats
- [ ] TF-IDF vectorization implemented with configurable parameters
- [ ] Cosine similarity correctly identifies similar documents
- [ ] Integration with Task 001 completed and tested
- [ ] Storage reclamation reporting accurate and comprehensive
- [ ] Unit tests with >85% coverage
- [ ] Integration tests with real document datasets
- [ ] Performance optimized for 1,000+ documents
- [ ] Memory usage profiled and optimized
- [ ] Documentation with examples and API reference
- [ ] CLI commands fully documented
- [ ] Code reviewed and approved

## Test Scenarios

1. **Identical documents with different names**: Should detect with >95% similarity
2. **Reformatted versions (spacing, line breaks)**: Should detect as similar
3. **Documents with minor edits**: Should detect based on threshold
4. **Different versions with substantial changes**: Should not flag as duplicates
5. **Empty or minimal text documents**: Should handle gracefully
6. **Corrupted or encrypted documents**: Should skip without errors
7. **Mixed language documents**: Should handle UTF-8 correctly
8. **Very large documents (>100MB)**: Should process within reasonable time
9. **Large collections (1,000+ files)**: Memory efficiency and performance
10. **Integration with image and hash deduplication**: Unified workflow

## Storage Reclamation Report Format

```
=== Storage Reclamation Report ===

Total Files Scanned: 1,234
Total Size: 45.6 GB

Duplicate Categories:
  Exact Duplicates (hash): 156 files, 2.3 GB
  Similar Images (pHash): 89 files, 1.8 GB
  Similar Documents (semantic): 67 files, 892 MB

Total Potential Savings: 5.0 GB (11% of scanned data)

Top Duplicate Groups:
  1. project_proposal_v*.docx (12 files, 456 MB)
  2. IMG_2023*.jpg (23 files, 678 MB)
  3. meeting_notes*.pdf (8 files, 234 MB)

Recommended Actions:
  - Review and merge document versions
  - Keep best quality images
  - Archive or delete old versions
```
