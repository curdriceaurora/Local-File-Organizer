---
name: optimize-semantic-similarity-performance
title: Optimize semantic similarity computation from O(n²) to vectorized approach
status: open
created: 2026-01-21T09:16:35Z
updated: 2026-01-21T09:22:45Z
size: M
hours: 8
parallel: false
dependencies: []
---

# Issue #68: Optimize semantic similarity computation

## Description

The semantic similarity computation uses a quadratic O(n²) algorithm that compares every document pair individually. This needs to be optimized using vectorized operations with scikit-learn or NumPy for better performance with large document sets.

## Current Problem

- Performance degrades significantly with large document sets
- 1,000 documents = ~500,000 comparisons
- 10,000 documents = ~50,000,000 comparisons

## Proposed Solution

Use vectorized operations:
- Compute TF-IDF matrix once
- Use cosine_similarity() for all pairs at once
- Target: O(n log n) or better

## Acceptance Criteria

- [ ] Vectorized similarity computation implemented
- [ ] Performance benchmarks show improvement
- [ ] Accuracy maintained (results match current implementation)
- [ ] Tests pass for edge cases (empty docs, single doc)

## Technical Details

**File**: `src/file_organizer/services/deduplication/semantic.py` (lines 94-111)
**Priority**: Medium
**Expected Impact**: 10-100x speedup for large datasets

## References

- CodeRabbit PR #67 review comment
- Related to Phase 4 deduplication features
