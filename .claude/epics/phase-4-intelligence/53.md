---
name: task-007-operation-history-tracking
title: Design and implement operation history tracking
epic: phase-4-intelligence
github_issue: null
status: open
size: L
hours: 24
parallel: true
depends_on: []
created: 2026-01-21T05:53:08Z
updated: 2026-01-26T00:52:32Z
assignee: null
labels: [task, phase-4, database, history-tracking]
github: https://github.com/curdriceaurora/Local-File-Organizer/issues/222
---

# Task 007: Design and implement operation history tracking

**Epic:** Phase 4 - Intelligence & Learning
**Size:** Large (24 hours)
**Can run in parallel:** Yes
**Dependencies:** None

## Objective
Build a robust operation history tracking system using SQLite to record all file operations, enabling undo/redo functionality and audit trails.

## Requirements

### 1. SQLite Database Schema
- **operations table**: Store all file operations
  - id (PRIMARY KEY)
  - operation_type (move, rename, delete, copy)
  - timestamp (ISO 8601)
  - source_path
  - destination_path
  - file_hash (SHA256)
  - metadata (JSON: file size, type, etc.)
  - transaction_id (for batch operations)
  - status (pending, completed, failed, rolled_back)

- **transactions table**: Track batch operations
  - transaction_id (PRIMARY KEY)
  - started_at
  - completed_at
  - operation_count
  - status (in_progress, completed, failed, partially_rolled_back)

- **indexes**: Optimize queries
  - Index on timestamp
  - Index on transaction_id
  - Index on operation_type
  - Index on status

### 2. Operation Tracking Features
- **Automatic logging**: All file operations logged automatically
- **Transaction support**: Group related operations together
- **Metadata capture**: File size, type, permissions, modification time
- **File hashing**: SHA256 hash for verification
- **Error tracking**: Log failures with error messages
- **Atomic operations**: Ensure database consistency

### 3. History Management
- **Configurable limits**:
  - Max operations (default: 10,000)
  - Max age (default: 90 days)
  - Max database size (default: 100 MB)
- **Automatic cleanup**: Purge old operations
- **Manual cleanup**: User-triggered history clear
- **History export**: Export operations to JSON/CSV

### 4. Persistence & Performance
- **Database location**: `~/.file_organizer/history.db`
- **Connection pooling**: Reuse database connections
- **Write batching**: Batch inserts for performance
- **Read optimization**: Prepared statements, indexes
- **Crash recovery**: Handle incomplete transactions
- **Concurrent access**: Handle multiple processes safely

## Technical Approach

### Database Schema (SQLite)
```sql
CREATE TABLE operations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    operation_type TEXT NOT NULL,
    timestamp TEXT NOT NULL,
    source_path TEXT NOT NULL,
    destination_path TEXT,
    file_hash TEXT,
    metadata TEXT,
    transaction_id TEXT,
    status TEXT NOT NULL DEFAULT 'completed',
    error_message TEXT,
    created_at TEXT DEFAULT (datetime('now'))
);

CREATE TABLE transactions (
    transaction_id TEXT PRIMARY KEY,
    started_at TEXT NOT NULL,
    completed_at TEXT,
    operation_count INTEGER DEFAULT 0,
    status TEXT NOT NULL DEFAULT 'in_progress',
    metadata TEXT
);

CREATE INDEX idx_operations_timestamp ON operations(timestamp);
CREATE INDEX idx_operations_transaction ON operations(transaction_id);
CREATE INDEX idx_operations_type ON operations(operation_type);
CREATE INDEX idx_operations_status ON operations(status);
CREATE INDEX idx_transactions_status ON transactions(status);
```

### Python Implementation Structure
```python
# file_organizer_v2/src/file_organizer/history/
├── database.py           # SQLite connection & schema
├── tracker.py            # Operation tracking logic
├── transaction.py        # Transaction management
├── cleanup.py            # History cleanup utilities
└── models.py             # Data models for operations
```

### Key Classes
```python
class OperationHistory:
    """Main interface for operation tracking"""
    def log_operation(operation_type, source, destination, metadata)
    def start_transaction() -> transaction_id
    def commit_transaction(transaction_id)
    def rollback_transaction(transaction_id)
    def get_operations(filters) -> List[Operation]
    def cleanup_old_operations(max_age_days)

class Operation:
    """Represents a single file operation"""
    id: int
    operation_type: str
    timestamp: datetime
    source_path: Path
    destination_path: Path
    file_hash: str
    metadata: dict
    transaction_id: str
    status: str

class Transaction:
    """Context manager for batch operations"""
    def __enter__() -> transaction_id
    def __exit__()
    def add_operation(operation)
    def commit()
    def rollback()
```

## Implementation Steps

1. **Database Setup** (4 hours)
   - Create SQLite schema
   - Implement connection manager
   - Add migration support
   - Test database operations

2. **Operation Tracking** (8 hours)
   - Implement OperationHistory class
   - Add logging for move operations
   - Add logging for rename operations
   - Add logging for delete operations
   - Add logging for copy operations
   - Test operation logging

3. **Transaction Support** (6 hours)
   - Implement Transaction class
   - Add context manager support
   - Test nested transactions
   - Test rollback scenarios

4. **History Management** (4 hours)
   - Implement cleanup logic
   - Add configurable limits
   - Add export functionality
   - Test history rotation

5. **Integration & Testing** (2 hours)
   - Integrate with file operations
   - Performance testing
   - Concurrent access testing
   - Documentation

## Acceptance Criteria

- [ ] SQLite database created with proper schema
- [ ] All file operations are logged automatically
- [ ] Transaction support for batch operations works correctly
- [ ] Configurable history limits (operations, age, size)
- [ ] History persists across application restarts
- [ ] Cleanup automatically removes old operations
- [ ] Database handles concurrent access safely
- [ ] Performance: Log 1000 operations in < 1 second
- [ ] Export history to JSON/CSV works
- [ ] Unit tests cover >90% of code
- [ ] Documentation complete

## Testing Strategy

### Unit Tests
- Database schema creation
- CRUD operations
- Transaction commit/rollback
- Cleanup logic
- Export functionality

### Integration Tests
- Log operations during file moves
- Batch operations in transactions
- History persistence across restarts
- Concurrent access from multiple processes

### Performance Tests
- Bulk insert performance (10k operations)
- Query performance with large datasets
- Database size growth over time

## Files to Create/Modify

### New Files
- `file_organizer_v2/src/file_organizer/history/__init__.py`
- `file_organizer_v2/src/file_organizer/history/database.py`
- `file_organizer_v2/src/file_organizer/history/tracker.py`
- `file_organizer_v2/src/file_organizer/history/transaction.py`
- `file_organizer_v2/src/file_organizer/history/cleanup.py`
- `file_organizer_v2/src/file_organizer/history/models.py`
- `file_organizer_v2/tests/history/test_database.py`
- `file_organizer_v2/tests/history/test_tracker.py`
- `file_organizer_v2/tests/history/test_transaction.py`

### Modified Files
- `file_organizer_v2/src/file_organizer/core/organizer.py` (integrate tracking)
- `file_organizer_v2/src/file_organizer/services/file_service.py` (add logging)

## Dependencies
- sqlite3 (Python standard library)
- Standard Python libraries: pathlib, datetime, json, hashlib

## Risks & Mitigations

**Risk:** Database corruption
**Mitigation:** Write-ahead logging (WAL), regular backups, crash recovery

**Risk:** Performance degradation with large history
**Mitigation:** Indexes, cleanup policies, query optimization

**Risk:** Concurrent access conflicts
**Mitigation:** Connection pooling, proper locking, transaction isolation

**Risk:** Disk space usage
**Mitigation:** Configurable limits, automatic cleanup, compression

## Notes
- Use WAL mode for better concurrent access
- Consider vacuum operations for database maintenance
- File hashes are optional but recommended for verification
- Transaction IDs use UUID for uniqueness
- All timestamps use ISO 8601 format in UTC

## Related Tasks
- Task 008: Build undo/redo functionality (depends on this task)

## References
- SQLite documentation: https://www.sqlite.org/docs.html
- Python sqlite3 module: https://docs.python.org/3/library/sqlite3.html
